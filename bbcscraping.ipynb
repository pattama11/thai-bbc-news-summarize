{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import numpy as np\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(web):\n",
    "    url = []\n",
    "    res = req.get(web).content\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    div = soup.find('div',class_='ssrcss-17ehax8-Cluster e1ihwmse1')\n",
    "    li = div.find_all('li',class_ = 'ssrcss-hp3otd-StyledListItem-PageButtonListItem e1ksme8n1')\n",
    "    for e in li:\n",
    "        num = e.find('div',class_ = 'ssrcss-3vkeha-StyledButtonContent e1kcrsdk1').text\n",
    "    for i in range(1,int(num)+1):\n",
    "        url.append(web + '?page=' + str(i))\n",
    "    print('Get number of pages successfully')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([item['data-component'] for item in soup.find_all() if 'data-component' in item.attrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get content of each news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(link):\n",
    "    news = ''\n",
    "    res = req.get(link).content\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    h = soup.find('h1', id = 'main-heading')\n",
    "    if '/sport' in str(link) or '/programmes' in str(link) or '/live' in str(link):\n",
    "        print('Special Contents')\n",
    "    else:\n",
    "        if h != None:\n",
    "            title = h.text\n",
    "            ul = soup.find_all('div',class_=\"ssrcss-11r1m41-RichTextComponentWrapper ep2nwvo0\")\n",
    "            if len(ul) == 0:\n",
    "                ul = soup.find_all('div',class_='ssrcss-1s1kjo7-RichTextContainer e5tfeyi1')\n",
    "                if len(ul) == 0:\n",
    "                    ul = soup.find_all('div',class_='ssrcss-7uxr49-RichTextContainer e5tfeyi1')\n",
    "            for i in ul:news = news + i.text\n",
    "            # print('Get news successfully')\n",
    "        else:\n",
    "            print('Can not find title in this news ' + link)\n",
    "\n",
    "    return title, news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get links in the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_news(web):\n",
    "    print('Getting links to news...')\n",
    "    link = []\n",
    "    res = req.get(web).content\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    ul = soup.find('ul', class_='ssrcss-w3duc2-SimpleGrid e52bkgs0')\n",
    "    li = ul.find_all('li')\n",
    "    for i in li:\n",
    "        if i.find('a', class_='ssrcss-1mrs5ns-PromoLink exn3ah91') != None:\n",
    "            l = i.find('a', class_='ssrcss-1mrs5ns-PromoLink exn3ah91').get('href')\n",
    "            # if 'sounds/play/' not in l:\n",
    "            if 'sounds/play/' not in l and ('/sport' not in str(l) and '/programmes' not in str(l) and '/live' not in str(l)):\n",
    "                link.append('https://www.bbc.com/'+l)\n",
    "    print('Get links to news successfully')\n",
    "    return link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get number of pages successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n",
      "Getting links to news...\n",
      "Get links to news successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Thais caught up in the Israel-Gaza war</td>\n",
       "      <td>In a village lying close to the Mekong River, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Catfishing: How I hunted down the gang imperso...</td>\n",
       "      <td>James Blake, an entrepreneur and owner of a di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamas attack: 12 Thais killed and 11 kidnapped...</td>\n",
       "      <td>Twelve Thais have been killed and another 11 k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bangkok: Parents of Siam Paragon mall shooter ...</td>\n",
       "      <td>The parents of a 14-year-old boy who fatally s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bangkok: Two dead and 14-year-old held over Si...</td>\n",
       "      <td>A 14-year-old boy has been arrested after two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Thai princess leaves $40,000 custom toilet 'un...</td>\n",
       "      <td>A toilet that cost an estimated $40,000 (£28,3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Missing backpacker Grace Taylor found in Thailand</td>\n",
       "      <td>A British backpacker has been found after goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Grace Taylor missing: Family of UK woman in Th...</td>\n",
       "      <td>A 21-year-old woman from Swanage, Dorset is mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Bangkok airport safety issues 'must be addressed'</td>\n",
       "      <td>The airline industry has called on the Thai go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Bangkok Erawan shrine bomb: Uighur suspects pl...</td>\n",
       "      <td>Two ethnic Uighur Chinese men have pleaded not...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0           The Thais caught up in the Israel-Gaza war   \n",
       "1    Catfishing: How I hunted down the gang imperso...   \n",
       "2    Hamas attack: 12 Thais killed and 11 kidnapped...   \n",
       "3    Bangkok: Parents of Siam Paragon mall shooter ...   \n",
       "4    Bangkok: Two dead and 14-year-old held over Si...   \n",
       "..                                                 ...   \n",
       "888  Thai princess leaves $40,000 custom toilet 'un...   \n",
       "889  Missing backpacker Grace Taylor found in Thailand   \n",
       "890  Grace Taylor missing: Family of UK woman in Th...   \n",
       "891  Bangkok airport safety issues 'must be addressed'   \n",
       "892  Bangkok Erawan shrine bomb: Uighur suspects pl...   \n",
       "\n",
       "                                              contents  \n",
       "0    In a village lying close to the Mekong River, ...  \n",
       "1    James Blake, an entrepreneur and owner of a di...  \n",
       "2    Twelve Thais have been killed and another 11 k...  \n",
       "3    The parents of a 14-year-old boy who fatally s...  \n",
       "4    A 14-year-old boy has been arrested after two ...  \n",
       "..                                                 ...  \n",
       "888  A toilet that cost an estimated $40,000 (£28,3...  \n",
       "889  A British backpacker has been found after goin...  \n",
       "890  A 21-year-old woman from Swanage, Dorset is mi...  \n",
       "891  The airline industry has called on the Thai go...  \n",
       "892  Two ethnic Uighur Chinese men have pleaded not...  \n",
       "\n",
       "[893 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web = 'https://www.bbc.com/news/topics/c77jz3md4vdt'\n",
    "title = []\n",
    "news = []\n",
    "pages = get_pages(web=web)\n",
    "for i in pages:\n",
    "    # print('Page' + str(i))\n",
    "    link = get_link_news(i)\n",
    "    for j in link:\n",
    "        title.append(get_news(j)[0])\n",
    "        news.append(get_news(j)[1])\n",
    "df = pd.DataFrame({'title':title,'contents' : news})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_messy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
